# ETLAgent
# GenAI Agentâ€“Driven ETL Pipeline (PoC)

## ğŸ“Œ Overview

This project is a **proof of concept (PoC)** that demonstrates how a **GenAI-powered agent** can plan, reason, and orchestrate an **ETL (Extractâ€“Transformâ€“Load) pipeline**.

The goal is **not** to build a production-grade system, but to showcase:

* Understanding of **GenAI / agent concepts**
* How LLMs can be applied to **data engineering problems**
* Clear separation between **reasoning (GenAI)** and **execution (deterministic tools)**

This implementation was designed to satisfy the requirements of a *GenAI Agentâ€“Driven ETL* interview assignment.

---

## ğŸ§  Key Idea

> **GenAI is used for reasoning and planning, not for data processing.**

* The **GenAI agent** reasons over dataset metadata and produces an ETL plan
* **Python, Pandas, and SQLite** execute the plan deterministically
* Rule-based guardrails handle schema drift and data quality

---

## ğŸ—ï¸ Architecture Overview

```
CSV / Source Data
        â†“
Metadata Extraction (Pandas)
        â†“
Schema Drift Detection
        â†“
GenAI Planning Agent (LLM)
        â†“
Plan Adjustment (Data Quality Rules)
        â†“
Transformation Execution (Pandas)
        â†“
Load to SQLite
```

---

## ğŸ“‚ Project Structure

```
ETLAgent/
â”‚
â”œâ”€â”€ main.py                  # Orchestrates the ETL pipeline
â”‚
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ planner.py           # GenAI + agent reasoning logic
â”‚   â””â”€â”€ schema.py            # Expected schema definition
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extractor.py         # Metadata extraction + schema drift detection
â”‚   â”œâ”€â”€ transformer.py       # Executes transformations
â”‚   â”œâ”€â”€ loader.py            # Loads data to SQLite
â”‚   â””â”€â”€ quality.py           # Data quality scoring
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ orders.csv           # Sample input data
â”‚
â””â”€â”€ README.md
```

---

## ğŸ¤– Role of GenAI (LLM)

The GenAI agent is responsible for:

* Understanding dataset **structure and schema** (via metadata)
* Deciding **validations** and **transformations**
* Generating an **ETL execution plan** in structured JSON
* Adapting decisions when data quality degrades

The LLM **does not**:

* Read raw data rows
* Execute transformations
* Replace Pandas or SQL

This separation ensures reliability, auditability, and safety.

---

## ğŸ” Agent Reasoning Flow

1. **Extract Metadata**

   * Columns
   * Data types
   * Null counts
   * Sample rows

2. **Detect Schema Drift**

   * Missing columns
   * New columns
   * Type mismatches

3. **GenAI Planning**

   * Produces ETL plan (JSON)
   * Defines validations, transformations, and load strategy

4. **Data Quality Adjustment**

   * Low quality â†’ stricter transformations

5. **Execution**

   * Pandas applies transformations
   * SQLite stores final data

---

## ğŸ§ª Data Quality Handling

A simple data quality score is calculated using null counts.

* High quality â†’ standard plan
* Low quality â†’ additional cleaning steps added by the agent

This demonstrates **adaptive behavior**, one of the key evaluation criteria.

---

## ğŸ§± Schema Drift Handling

The pipeline detects:

* Missing columns â†’ pipeline fails
* New columns â†’ accepted automatically
* Type mismatches â†’ agent decides casting strategy

This logic is implemented using **rule-based guardrails** around the GenAI agent.

---

## â–¶ï¸ How to Run

### 1. Create virtual environment

```bash
python -m venv venv
venv\Scripts\activate
```

### 2. Install dependencies

```bash
python -m pip install pandas openai
```

### 3. (Optional) Set OpenAI API key

```bash
setx OPENAI_API_KEY "your_api_key_here"
```

> If no API key is provided, the pipeline automatically falls back to a mock planning agent.

### 4. Run the pipeline

```bash
python main.py
```

---

## ğŸ” Safety & Reliability

* GenAI output is constrained to **strict JSON**
* `eval()` is never used
* Deterministic fallback logic ensures pipeline reliability
* Execution is fully controlled by Python tools

---

## âš–ï¸ Trade-offs & Limitations

**Pros**

* Flexible, adaptive ETL planning
* Clear agent-based design
* Easy to extend to new data sources

**Limitations**

* Not suitable for large-scale data
* LLM responses may vary
* Added latency and cost from GenAI calls

---

## ğŸš€ Future Enhancements

* Support for multiple data sources (API, JSON)
* More advanced data quality metrics
* Integration with workflow orchestrators (Airflow)
* Logging and monitoring for agent decisions

---

## ğŸ¯ Summary

This project demonstrates how **GenAI agents can enhance ETL pipelines** by introducing reasoning, planning, and adaptabilityâ€”while keeping execution deterministic and safe.

It directly addresses the evaluation criteria of the assignment and serves as a clear, interview-ready example of **GenAI applied to data engineering**.
